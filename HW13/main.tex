\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 13}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{Finite $\Theta$.}

\begin{subquestion}{Verify that $\bar{p}$ is a probability mass function on $\{ 0, 1 \}^m$.}
\begin{solution}
\begin{align*}
\sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \bar{p}(z_1, \dots, z_m) & = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} w(\theta) p_{\theta} (z^m) \\
& = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} \frac{1}{N} p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta}  \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m}  p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta} 1 \\
& = \frac{1}{N} N \\
& = 1
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Show that the worst-case case regret of $\bar{p}$ is also bounded by $\log N$.}
\begin{solution}
\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& =  - \log \bar{p} (z_1 | \epsilon) - \cdots - \log \bar{p} (z_m | z^{m -1}) \\
& =  - (\log \bar{p} (z_1 | \epsilon) + \cdots + \log \bar{p} (z_m | z^{m -1})) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} + \cdots + \log  \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} \cdot \cdots \cdot \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^m)}{\bar{p} (\epsilon)} \right) \\
& =  - \log \bar{p} (z^m) \\
\end{align*}

\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& \geq \max_{i \in \{1, \dots, m\}} - \log \bar{p} (z_i | z^{i -1}) \\
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Let us fix $N = |\Theta|$ and set $\Theta = \{1/(N + 1), . . . , N/(N + 1) \}$. For example, if we set
$N = 4$ then $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
/
\end{solution}
\end{subquestion}

\begin{subquestion}{Use the law of large numbers to argue that
\[
\lim_{m \to \infty} S_m = \log N.
\]}
\begin{solution}
\end{solution}
\end{subquestion}

\begin{subquestion}{Informally explain why, for small sample sizes m, the Shtarkov sum for $\Theta = \{ 0.47, 0.49, 0.51, 0.53 \}$ is significantly smaller than the Shtarkov sum for $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
Because elements of $\Theta$ are much closer to each other. 
\end{solution}
\end{subquestion}

\begin{subquestion}{Suppose that $P$ consists of a finite number of black-box experts, which given each history $Z_1, Z_{i-1}$ provide us a distribution on $Z_i$, which may depend on the past --- we don’t know how they come up with their predictions, we just observe the predictions they make on the sample. Explain why we’d rather use the Bayesian than the Shtarkov predictor in such a setting.}
\begin{solution}
\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}
{(Uncountable $\Theta$, $[4+1/3$ pt $]$) Now consider the full Bernoulli model, $\Theta=[0,1]$, with the sum in $(1)$ repladed by an integral, for the uniform prior probability densiwty $w)\theta ):=1$ for all $\theta \in [0,1]$.
\begin{subquestion}{For each of the follwoing statements, indicate wheter it is true or false, and prove your answer.
\begin{enumerate}
\item $[3+1/3 pt]$ The NML predictor $p^*$ achieves the same regret for every sequence $x^m-x_1, \dots , x_m \in \{ 0,1 \} ^m$.
\item same as (i) with 'regret' replaced by 'cumulative loss'.
\item the Bayesian predictor $\bar{p}$ achieves the same regret for every sequence $x^m \in \{ 0,1 \} ^m$.
\item same as (iii) with 'regret' replaced by 'cumulative loss'.
\item for every fixed $x^m \in \{ 0,1 \} ^m$, the NML predictor $p^*$ achieves the same regret on every $y^m$ that is a permutation of $x^m$.
\item same as (vii) with 'regret' replaced by 'cumulative loss'.
\item for every fixed $x^m \in \{ 0,1 \} ^m$, every $n<m$, the NML predictor $p^*$ achieves the same loss on $y_1 , \dots , y_n$, for every $y_1, \dots , y_n$ that are the initial segment of an $y_1 , \dots , y_m$ that is a permutation of $x^m$.
\item same as (ix) but with $p^*$ replaced by $\bar{p}$.
\end{enumerate}
}
\end{subquestion}
\begin{subquestion}{
[1 pt] Suppose we do not know in advance how many predictions we have to make, i.e. we do not know the \textit{horizon} $n$. Explain why this is a problem for prediction with the NML $p^*$ but not for prediction with $\bar{p}$.}
\end{subquestion}
}
\end{question}


\bibliographystyle{plain}
\bibliography{references}
\end{document}



















