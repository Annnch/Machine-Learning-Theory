\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 13}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{Finite $\Theta$.}

\begin{subquestion}{Verify that $\bar{p}$ is a probability mass function on $\{ 0, 1 \}^m$.}
\begin{solution}
\begin{align*}
\sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \bar{p}(z_1, \dots, z_m) & = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} w(\theta) p_{\theta} (z^m) \\
& = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} \frac{1}{N} p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta}  \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m}  p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta} 1 \\
& = \frac{1}{N} N \\
& = 1
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Show that the worst-case case regret of $\bar{p}$ is also bounded by $\log N$.}
\begin{solution}
\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& =  - \log \bar{p} (z_1 | \epsilon) - \cdots - \log \bar{p} (z_m | z^{m -1}) \\
& =  - (\log \bar{p} (z_1 | \epsilon) + \cdots + \log \bar{p} (z_m | z^{m -1})) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} + \cdots + \log  \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} \cdot \cdots \cdot \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^m)}{\bar{p} (\epsilon)} \right) \\
& =  - \log \bar{p} (z^m) \\
\end{align*}

\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& \geq \max_{i \in \{1, \dots, m\}} - \log \bar{p} (z_i | z^{i -1}) \\
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Let us fix $N = |\Theta|$ and set $\Theta = \{1/(N + 1), . . . , N/(N + 1) \}$. For example, if we set
$N = 4$ then $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
/
\end{solution}
\end{subquestion}

\begin{subquestion}{Use the law of large numbers to argue that
\[
\lim_{m \to \infty} S_m = \log N.
\]}
\begin{solution}
\end{solution}
\end{subquestion}

\begin{subquestion}{Informally explain why, for small sample sizes m, the Shtarkov sum for $\Theta = \{ 0.47, 0.49, 0.51, 0.53 \}$ is significantly smaller than the Shtarkov sum for $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
Because elements of $\Theta$ are much closer to each other. 
\end{solution}
\end{subquestion}

\begin{subquestion}{Suppose that $P$ consists of a finite number of black-box experts, which given each history $Z_1, Z_{i-1}$ provide us a distribution on $Z_i$, which may depend on the past --- we don’t know how they come up with their predictions, we just observe the predictions they make on the sample. Explain why we’d rather use the Bayesian than the Shtarkov predictor in such a setting.}
\begin{solution}
\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}
{(Uncountable $\Theta$, $[4+1/3$ pt $]$) Now consider the full Bernoulli model, $\Theta=[0,1]$, with the sum in $(1)$ repladed by an integral, for the uniform prior probability densiwty $w(\theta ):=1$ for all $\theta \in [0,1]$.
\begin{subquestion}{For each of the follwoing statements, indicate wheter it is true or false, and prove your answer.
\begin{enumerate}
% 1
\item $[3+1/3 pt]$ The NML predictor $p^*$ achieves the same regret for every sequence $x^m-x_1, \dots , x_m \in \{ 0,1 \} ^m$.
\begin{solution}
True. \\
\[
p_n^*(x^n) = \frac{\sup_{\theta \in \Theta} \theta_n(x^n)}{\sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n(z^n)}
\]
\begin{align*}
\hat{L}(x^n) - \inf_{\theta \in \Theta} L_\theta (x^n) & = \log \frac{\sup_{\theta \in \Theta} \theta_n(x^n)}{p_n^* (x^n)} \\
& = \log \sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n (z^n) \\ 
& \Rightarrow \text{independant of $x^n$} \\
& \Rightarrow \text{regret is the same for $\forall x^n$}
\end{align*}
\end{solution}
% 2
\item same as (i) with 'regret' replaced by 'cumulative loss'.
\begin{solution}
False. \\
\[
p_n^*(x^n) = \frac{\sup_{\theta \in \Theta} \theta_n(x^n)}{\sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n(z^n)}
\]
\begin{align*}
\hat{L}(x^n) & = \log \frac{1}{p_n^* (x^n)} \\
& = \log \frac{\sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n (z^n)}{\sup_{\theta \in \Theta} \theta_n(x^n)} \\ 
& \Rightarrow \text{dependant on $x^n$} \\
& \Rightarrow \text{cumulative loss is not the same for $\forall x^n$}
\end{align*}
\end{solution}
% 3
\item the Bayesian predictor $\bar{p}$ achieves the same regret for every sequence $x^m \in \{ 0,1 \} ^m$.
\begin{solution}
False. \\
\[
\bar{p}_n(x^n) = \int w(\theta) p_\theta (x^n) \; d\theta = \int p_\theta (x^n) \; d\theta 
\]
\begin{align*}
\hat{L}(x^n) - \inf_{\theta \in \Theta} L_\theta (x^n) & = \log \frac{\sup_{\theta \in \Theta} \theta_n(x^n)}{\bar{p}_n (x^n)} \\
& = \log \frac{\sup_{\theta \in \Theta} \theta_n(x^n)}{\int p_\theta (x^n) \; d\theta } \\
& \Rightarrow \text{dependant on $x^n$} \\
& \Rightarrow \text{regret is the not the same for $\forall x^n$}
\end{align*}
\end{solution}
% 4
\item same as (iii) with 'regret' replaced by 'cumulative loss'.
\begin{solution}
False. \\
\[
\bar{p}_n(x^n) = \int w(\theta) p_\theta (x^n) \; d\theta = \int p_\theta (x^n) \; d\theta 
\]
\begin{align*}
\hat{L}(x^n) & = \log \frac{1}{\bar{p}_n (x^n)} \\
& = \log \frac{1}{\int p_\theta (x^n) \; d\theta} \\
& \Rightarrow \text{dependant on } x^n \\
& \Rightarrow \text{not the same for } \forall x^n
\end{align*}
\end{solution}
% 5
\item for every fixed $x^m \in \{ 0,1 \} ^m$, the NML predictor $p^*$ achieves the same regret on every $y^m$ that is a permutation of $x^m$.
\begin{solution}
True. \\
Since the regret is the same for every $x^n$, it is also the same for the permutation of $x^n$.
\end{solution}
% 6
\item same as (vii) with 'regret' replaced by 'cumulative loss'.
\begin{solution}
True.
\begin{align*}
\hat{L}(x^n) & = \log \frac{1}{p_n^* (x^n)} \\
& = \log \frac{\sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n (z^n)}{\sup_{\theta \in \Theta} \theta_n(x^n)} \\ 
& \Rightarrow \sup_{\theta \in \Theta} \theta_n(x^n) \text{ is independant on the order of the elements of $x^n$} \\
& \Rightarrow \text{cumulative loss is the same for every permutation of $x^n$}
\end{align*}
\end{solution}
% 7
\item for every fixed $x^m \in \{ 0,1 \} ^m$, every $n<m$, the NML predictor $p^*$ achieves the same loss on $y_1 , \dots , y_n$, for every $y_1, \dots , y_n$ that are the initial segment of an $y_1 , \dots , y_m$ that is a permutation of $x^m$.
\begin{solution}
False.
\begin{align*}
\hat{L}(y^n) = \log \frac{\sum_{z^n \in Z^n} \sup_{\theta \in \Theta} \theta_n (z^n)}{\sup_{\theta \in \Theta} \theta_n(y^n)} \\ 
\end{align*}
$\sup_{\theta \in \Theta} \theta_n(y^n)$ can be different for every $y^n$, therefore the loss is not the same for every $y^n$.
\end{solution}
% 8
\item same as (ix) but with $p^*$ replaced by $\bar{p}$.
\begin{solution}
False.
\begin{align*}
\hat{L}(y^n) = \log \frac{1}{\int p_\theta (y^n) \; d\theta} \\
\end{align*}
$\int p_\theta (y^n) \; d\theta$ can be different for every $y^n$, therefore the loss is not the same for every $y^n$.
\end{solution}
\end{enumerate}
}
\end{subquestion}
\begin{subquestion}{
[1 pt] Suppose we do not know in advance how many predictions we have to make, i.e. we do not know the \textit{horizon} $n$. Explain why this is a problem for prediction with the NML $p^*$ but not for prediction with $\bar{p}$.}
\end{subquestion}
}
\end{question}


\bibliographystyle{plain}
\bibliography{references}
\end{document}



















