\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 13}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{Finite $\Theta$.}

\begin{subquestion}{Verify that $\bar{p}$ is a probability mass function on $\{ 0, 1 \}^m$.}
\begin{solution}
\begin{align*}
\sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \bar{p}(z_1, \dots, z_m) & = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} w(\theta) p_{\theta} (z^m) \\
& = \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m} \sum_{\theta \in \Theta} \frac{1}{N} p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta}  \sum_{(z_1, \dots, z_m) \in \{ 0, 1 \}^m}  p_{\theta} (z^m) \\
& = \frac{1}{N}  \sum_{\theta \in \Theta} 1 \\
& = \frac{1}{N} N \\
& = 1
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Show that the worst-case case regret of $\bar{p}$ is also bounded by $\log N$.}
\begin{solution}
\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& =  - \log \bar{p} (z_1 | \epsilon) - \cdots - \log \bar{p} (z_m | z^{m -1}) \\
& =  - (\log \bar{p} (z_1 | \epsilon) + \cdots + \log \bar{p} (z_m | z^{m -1})) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} + \cdots + \log  \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^1)}{\bar{p} (\epsilon)} \cdot \cdots \cdot \frac{\bar{p} (z^m)}{\bar{p} (z^{m - 1})} \right) \\
& =  - \left( \log \frac{\bar{p} (z^m)}{\bar{p} (\epsilon)} \right) \\
& =  - \log \bar{p} (z^m) \\
\end{align*}

\begin{align*}
- \log \bar{p}(z^m) & = \sum_{i = 1}^m - \log \bar{p} (z_i | z^{i -1}) \\
& \geq \max_{i \in \{1, \dots, m\}} - \log \bar{p} (z_i | z^{i -1}) \\
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Let us fix $N = |\Theta|$ and set $\Theta = \{1/(N + 1), . . . , N/(N + 1) \}$. For example, if we set
$N = 4$ then $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
/
\end{solution}
\end{subquestion}

\begin{subquestion}{Use the law of large numbers to argue that
\[
\lim_{m \to \infty} S_m = \log N.
\]}
\begin{solution}
\end{solution}
\end{subquestion}

\begin{subquestion}{Informally explain why, for small sample sizes m, the Shtarkov sum for $\Theta = \{ 0.47, 0.49, 0.51, 0.53 \}$ is significantly smaller than the Shtarkov sum for $\Theta = \{ 0.2, 0.4, 0.6, 0.8 \}$.}
\begin{solution}
Because elements of $\Theta$ are much closer to each other. 
\end{solution}
\end{subquestion}

\begin{subquestion}{Suppose that $P$ consists of a finite number of black-box experts, which given each history $Z_1, Z_{i-1}$ provide us a distribution on $Z_i$, which may depend on the past --- we don’t know how they come up with their predictions, we just observe they predictions they make on the sample. Explain why we’d rather use the Bayesian than the Shtarkov predictor in such a setting.}
\begin{solution}
\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}{}
\end{question}

\bibliographystyle{plain}
\bibliography{references}
\end{document}



















