\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 8}
\author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
%\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{The Aggregating Algorithm plays $w_1^k = 1/K$ and updates as
\[
w_{t+1}^k = \frac{w_t^k e^{-l_t^k}}{\sum_{j = 1}^K w_t^j e^{-l_t^j}}
\]
Let us define the \emph{Kullback-Leibler divergence} aka \emph{relative entropy} (notion of distance between probability distributions) from $p \in \Delta_K$ to $q \in \Delta_K$ by
\[
\text{KL}(p. q) = \sum_{k = 1}^K p_k \ln \frac{p_k}{q_k}
\]
Fix $w_t \in \Delta_K$ and $l_t \in \mathbb{R}^K$. Consider the minimisation problem
\begin{equation}
\min_{w \in \Delta_K} w^T l_t + \text{KL}(w, w_t)
\label{eq: 1}
\end{equation}
}

\begin{subquestion}{Show that the minimiser of problem \eqref{eq: 1} is $w_{t + 1}$.}

\begin{solution}
We start of by writing down our Lagrangian, where the only constraint on the $k$-dimensional vector $w$ is that they sum up to be one, or written otherwise $\sum_{k=1} ^K w^k-1=0$. Therefore the Lagrangian is:
$$L(w,\lambda ) = w^Tj_t + \sum _{k=1} ^K w^k \ln \frac{w^k}{w_t^k} + \lambda (\sum _{k=1} ^K w^k - 1)$$
We find an optimal solution when both $\frac{\partial L}{\partial w^k}=0$ for all $k\in [K]$ and $\frac{\partial L}{\partial \lambda}=0$. The first one gives the equation
\begin{align*}
&l_t^k+ \ln \frac{w^k}{w_t^k}+1-\lambda =0\\
&\implies \frac{w^k}{w_t^k}=e^{\lambda -1-l_t^k}\\
&\implies w^k=w_t^k e^{\lambda -1-l_t^k}
\end{align*}
The second of the partial derivatives give $\frac{\partial L}{\partial \lambda}=\sum _{k=1} ^K w^k-1=0$. Substitution of the expression of $w^k$ we just found now gives $$\sum_{k=1}^Kw_t^ke^{\lambda-1-l+t^k}=1.$$
But this means that our first expression of $w_k$ can be written as
\begin{align*}
w^k&=\frac{w_t^ke^{\lambda-1l_t^k}}{1}\\
&=\frac{w_t^ke^{\lambda-1l_t^k}}{\sum_{k=1}^Kw_t^ke^{\lambda-1-l+t^k}}\\
&=\frac{w_t^ke^{-l_t^k}}{\sum _{k=1} ^K w_t^ke^{-l_t^k}}\\
&=w_{t+1}^k
\end{align*}
\end{solution}

\end{subquestion}

\begin{subquestion}{Show that the value of problem \eqref{eq: 1} is the mix loss.}

\begin{solution}
Now we only need to check that when we plug in the solution as found in 1a in the function we were ought to minimize, we obtain the mix loss. We have:
\begin{align*}
w_{t+1}^Tl_t+\text{KL}(w_{t+1},w_t)&=\sum _{k=1} ^K w_{t+1}^k l_t^k + \sum _{k=1}^K w_{t+1}^k \ln \frac{w^k_{t+1}}{w_t^k}\\
&=\sum _{k=1} ^K (w_{t+1}^k (l_t^k + \ln \frac{w_{t+1}^k}{w^k_t}))\\
&=\sum _{k=1} ^K \frac{w_t^ke^{-l_t^k}}{\sum _{k=1} ^K w_t^j e^{-l_t^j}}(l_t^k+\ln \frac{e^{-l_t^k}}{\sum _{k=1} ^K w_t^je^{-l_t^j}})\\
&=\sum _{k=1} ^K \frac{w_t^ke^{-l_t^k}}{\sum _{k=1} ^K w_t^j e^{-l_t^j}}(l_t^k- l_t^k - \ln (\sum _{j=1}^K w_t^je^{-l_t^j}))\\
&=-\ln (\sum _{j=1}^K w_t^je^{-l_t^j})
\end{align*}
which is exactly the mix loss.
\end{solution}

\end{subquestion}

\end{question}

% 2nd question
\begin{question}{We saw in the lecture that the Hedge algorithm (for the dot-loss game)
with learning rate $\eta = \sqrt{\frac{8 \ln K}{T}}$ has regret after $T$ rounds bounded by $\sqrt{T/2 \ln K}$. In practice, we may not know $T$ in advance, or we may even desire an algorithm that has good guarantees for all $T$ simultaneously, i.e.\ that keeps on operating forever. \\ Consider the following exponential (base 3) restarting schedule to accomplish
this. We run Hedge for 1 round, with $\eta$ tuned for 1 round. After that, we restart Hedge, and run it for 3 rounds with $\eta$ tuned for 3 rounds. After that, we restart Hedge again for 9 rounds with $\eta$ tuned for 9 rounds, and so on. \\ Prove that the overall accumulated regret of Hedge with this scheme is bounded above by a universal constant times $\sqrt{T \ln K}$.  (Your argument should work for $T$ that are not a power of 3).}

\begin{solution}
Any choice of $T$ is bounded by powers of 3 i nthe following way: $3^p\leq T < 3^{p+1}$. So by the result from the lecture, we have for the restarting schedule as stated in the exercise: 
\begin{align*}
R^T&\leq \sum _{i=1} ^{p+1} \sqrt{3^i/2\ln K}\\
&=\sum _{i=0} ^{p} \sqrt{3^{i+1}/2\ln K}\\
&=\sqrt{\frac{1}{2}}\sqrt{3^p \ln K } \sum _{i=0}^p \sqrt{3^{i+1-p}}\\
&=\sqrt{\frac{3}{2}}\sqrt{3^p \ln K } \sum _{i=0}^p \sqrt{3^{i-p}}\\
&=\sqrt{\frac{3}{2}}\sqrt{3^p \ln K } \sum _{i=0}^p \sqrt{3^{-i}}
\end{align*}
% http://www.cs.princeton.edu/~rlivni/cos511/lectures/lect18.pdf
% http://people.csail.mit.edu/costis/6896sp10/lec4.pdf
\end{solution}

\end{question}

% 3nd question
\begin{question}{Consider the $K = 2$ expert version of the $T$-round dot loss game (Definition 2). In this exercise we will prove that the worst-case expected regret is at least of order $\sqrt{T}$. Consider an adversary that for each $t = 1, \dots, T$ assigns loss vector $l_t = (0, 1)$ or $l_t = (1, 0)$ i.i.d\ uniformly at random.}

\begin{subquestion}{ Show that the expected loss of any learner is $T/2$.}

\begin{solution}
We calculate the dot loss as:
\[
\sum_{k = 1}^K w_t^k l_t^k
\]
Where, in our case:
\[
w_t \in \{ (0, 0), (0, 1), (1, 0), (1, 1) \} \text{ and } l_t \in \{ (0, 1), (1, 0) \}
\]
In the table below we can see all the possible values of $L_t = \sum_{k = 1}^2 w_t^k l_t^k$:
\[
\begin{array}{ c || c | c }
\displaystyle \sum_{k = 1}^2 w_t^k l_t^k & (0, 1) & (1, 0) \\ \hline \hline
(0, 0) & 0 & 0 \\ \hline
(0, 1) & 1 & 0 \\ \hline
(0, 1) & 0 & 1 \\ \hline
(1, 1) & 1 & 1
\end{array}
\]
From that it follows:
\[
P(L = 0) = P(L = 1) = \frac{1}{2}
\]
And we can conclude:
\begin{align*}
\mathbb{E} \left[ \sum_{t = 1}^T L_t \right] & = \sum_{t = 1}^T \mathbb{E}[L_t] \\
& = \sum_{t = 1}^T \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot 1 \\
& = \frac{T}{2}
\end{align*}
\end{solution}

\end{subquestion}

\begin{subquestion}{Show that $2(1/2 - l_t^k)$  is Rademacher for each $k \in \{1,2\}$.}

\begin{solution}
We know that $l_t^k$ can take two values: 0 or 1. So:
\begin{align*}
2(1/2 - l_t^k) & = 2(1/2 - 1) = -1 \\
2(1/2 - l_t^k) & = 2(1/2 - 0) = 1
\end{align*}
From that follows that $2(1/2 - l_t^k)$ takes values -1 or 1, therefore it is Rademacher.
\end{solution}

\end{subquestion}

\begin{subquestion}{ Show that $\sum_{t = 1}^T (1/2 - l_t^2) = - \sum_{t = 1}^T (1/2 - l_t^1)$.}

\begin{solution}
\begin{align*}
\sum_{t = 1}^T (1/2 - l_t^2) = - \sum_{t = 1}^T (1/2 - l_t^1) \\
\sum_{t = 1}^T (1/2 - l_t^2) + \sum_{t = 1}^T (1/2 - l_t^1) = 0 \\
\sum_{t = 1}^T (1/2 - l_t^2 + 1/2 - l_t^1) = 0 \\
\sum_{t = 1}^T (1/2 + 1/2 - (l_t^1 + l_t^2)) = 0 \\
\sum_{t = 1}^T (1 - 1) = 0 \\
0 = 0
\end{align*}
\end{solution}

\end{subquestion}

\begin{subquestion}{Argue that the expected loss of the best expert is bounded above by $\mathbb{E}[\min_k \sum_{t = 1}^T l_t^k] \leq T/2 - c\sqrt{T}$ for some $c > 0$. You can use the following fact. Let $X_1, \dots, X_T$ be i.i.d\ Rademacher random variables. Then 
\[
\mathbb{E} \left[ \sum_{t = 1}^T X_t \right] \in \left[ \sqrt{\frac{2(T - 1)}{\pi}}, \sqrt{\frac{2(T + 1)}{\pi}} \right].
\]
}

\begin{solution}
We know, that $2(1/2 - l_t^k)$ is Rademacher, so it holds:
\begin{align*}
\mathbb{E} \left[ \sum_{t = 1}^T 2(1/2 - l_t^2) \right] & \in \left[ \sqrt{\frac{2(T - 1)}{\pi}}, \sqrt{\frac{2(T + 1)}{\pi}} \right] \\
\mathbb{E} \left[ \sum_{t = 1}^T 1/2 - l_t^2 \right] & \in \left[ \sqrt{\frac{(T - 1)}{2\pi}}, \sqrt{\frac{(T + 1)}{2\pi}} \right] \\
\end{align*}
From the previous point:
\begin{align*}
\mathbb{E} \left[ - \sum_{t = 1}^T 1/2 - l_t^1 \right] & \in \left[ \sqrt{\frac{(T - 1)}{2\pi}}, \sqrt{\frac{(T + 1)}{2\pi}} \right] \\
\mathbb{E} \left[ \sum_{t = 1}^T 1/2 - l_t^1 \right] & \in \left[ - \sqrt{\frac{(T + 1)}{2\pi}}, - \sqrt{\frac{(T - 1)}{2\pi}} \right] \\
\end{align*}
It follows:
\begin{align*}
\mathbb{E} \left[ T/2 - \sum_{t = 1}^T l_t^1 \right] & \in \left[ - \sqrt{\frac{(T + 1)}{2\pi}}, - \sqrt{\frac{(T - 1)}{2\pi}} \right] \\
\mathbb{E} \left[ T/2 - \sum_{t = 1}^T l_t^2 \right] & \in \left[ \sqrt{\frac{(T - 1)}{2\pi}}, \sqrt{\frac{(T + 1)}{2\pi}} \right] \\
\end{align*}
$\Rightarrow$
\begin{align*}
\mathbb{E} \left[ - \sum_{t = 1}^T l_t^1 \right] & \in \left[ - \sqrt{\frac{(T + 1)}{2\pi}} - T/2, - \sqrt{\frac{(T - 1)}{2\pi}} -T/2 \right] \\
\mathbb{E} \left[ - \sum_{t = 1}^T l_t^2 \right] & \in \left[ \sqrt{\frac{(T - 1)}{2\pi}}  - T/2, \sqrt{\frac{(T + 1)}{2\pi}}  - T/2 \right] \\
\end{align*}
$\Rightarrow$
\begin{align*}
\mathbb{E} \left[ \sum_{t = 1}^T l_t^1 \right] & \in \left[ \sqrt{\frac{(T - 1)}{2\pi}} + T/2, \sqrt{\frac{(T + 1)}{2\pi}} + T/2 \right] \\
\mathbb{E} \left[ \sum_{t = 1}^T l_t^2 \right] & \in \left[ -\sqrt{\frac{(T + 1)}{2\pi}}  + T/2, -\sqrt{\frac{(T - 1)}{2\pi}}  + T/2 \right] \\
\end{align*}
From above we conclude $\mathbb{E}[\min_k \sum_{t = 1}^T l_t^k] = \mathbb{E}[\sum_{t = 1}^T l_t^2]$ and finally:
\[
\mathbb{E}[\min_k \sum_{t = 1}^T l_t^k] \leq T/2 - c\sqrt{T}
\]
\end{solution}
\end{subquestion}

\end{question}

\bibliographystyle{plain}
\bibliography{references}
\end{document}



















