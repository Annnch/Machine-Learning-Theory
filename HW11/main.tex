\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 11}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{\textbf{Mirror Descent and Continuous Exponential Weights} \\ In this exercise we look at Online Gradient Descent on $U = \mathbb{R}^d$, i.e. without any projections. Then Online Gradient Descent plays iterates $w_1 = 0$ and 
\begin{equation}
w_{t + 1} = w_t - \eta \nabla f_t (w_t)
\label{eq: 1}
\end{equation}}

\begin{subquestion}{Show that the OGD iterate $w_{t+1}$ is the minimiser of the problem
\[
\min_{w \in \mathbb{R}^d} \langle w, \nabla f_t (w) \rangle + \frac{1}{2 \eta} || w - w_t || ^2.
\]}
\begin{solution}
Let's define a function $g: \mathbb{R}^d \to \mathbb{R}$:
\begin{align*}
g(w) & = \langle w, \nabla f_t (w) \rangle + \frac{1}{2 \eta} || w - w_t || ^2 \\
& = w_1 \frac{\partial f_t}{\partial w_1} (w) + \cdots + w_d \frac{\partial f_t}{\partial w_d} (w) + \frac{1}{2 \eta} \left( (w_1 - w_{t1})^2 + \cdots (w_d - w_{td})^2 \right)
\end{align*}
We would like to find an extreme point, so $\frac{\partial g}{\partial w_i}(w^*) = 0;\ \forall i \in \{1, \dots, d\}$.
\[
\frac{\partial g}{\partial w_i} = \frac{\partial f_t}{\partial w_i} (w) + \frac{1}{\eta} (w_i - w_{ti})
\]
\begin{align*}
& \Rightarrow \qquad w_i^* = w_{ti} - \eta \frac{\partial f_t}{\partial w_i} (w) \\
& \Rightarrow \qquad w^* = w_{t} - \eta \nabla f_t (w)
\end{align*}
To show that calculated extreme is a minimum, we are going to show that $g$ is a convex function.
\end{solution}
\end{subquestion}

\begin{subquestion}{Next we look at Exponential Weights (with learning rate $\eta$) on the continuous space $\mathbb{R}^d$. We start with the spherical Gaussian prior density
\[
p_1 (u) = (2 \pi) ^{-d/2} e^{- \frac{||u||^2}{2}}
\]
and we update the density using the exponential weights update
\[
p_{t + 1} (u) = \frac{p_t (u) e^{- \eta \langle u, \nabla f_t (w_t) \rangle}}{\text{normalisation}}
\]
where we change each point $u \in \mathbb{R}$ the linearized loss $\langle u, \nabla f_t (w_t) \rangle$ (and not the actual loss $f_t (u)$). Let $\mu_t = \int_{\mathbb{R}^d} u p_t (u) du$ be the mean of $p_t$. Let $w_t$ be the iterates of Online Gradient Descent \eqref{eq: 1}. Show that $\mu_t = w_t$ for all $t$.}
\begin{solution}
In the following calculations we used two known integrals:
\begin{align*}
& \int_{-\infty}^\infty x e^{-a x^2 + b x} dx = \frac{\sqrt{\pi} b}{2 a^{3/2}} e^{\frac{b^2}{4a}}; \quad (\text{Re}(a) > 0) \\
& \int_{-\infty}^\infty e^{-a x^2} dx = \frac{1}{2}\sqrt{\frac{\pi}{a}}; \quad (a > 0)
\end{align*}
\allowdisplaybreaks
\begin{align*}
\int_{\mathbb{R}^d} u p_1(u) du & = \int_{\mathbb{R}^d} u\;  (2 \pi) ^{-d/2} e^{- \frac{||u||^2}{2}} du \\
&  =  (2 \pi) ^{-d/2} \int_{\mathbb{R}^d} u\;  e^{- \frac{u_1^2 + \cdots u_d^2}{2}} du \\
%
&  =  (2 \pi) ^{-d/2} \left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty u_1\;  e^{- \frac{u_1^2 + \cdots u_d^2}{2}} du_1 \dots du_d, \right. \\
& \qquad \qquad \qquad \dots, \\
& \qquad \qquad \qquad \left. \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty u_d\;  e^{- \frac{u_1^2 + \cdots u_d^2}{2}} du_1 \dots du_d \right) \\
%
&  =  (2 \pi) ^{-d/2} \left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty e^{- \frac{u_2^2 + \cdots u_d^2}{2}} \left( \int_{-\infty}^\infty u_1\;  e^{- \frac{u_1^2}{2}} du_1 \right) du_2 \dots du_d, \right. \\
& \qquad \qquad \qquad \dots, \\
& \qquad \qquad \qquad \left. \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty u_d\; e^{- \frac{u_2^2 + \cdots u_{d-1}^2}{2}} \left( \int_{-\infty}^\infty e^{- \frac{u_1^2}{2}} du_1 \right) du_2 \dots, du_d \right) \\
%
&  =  (2 \pi) ^{-d/2} \left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty e^{- \frac{u_2^2 + \cdots u_d^2}{2}} \left(  \frac{\sqrt{\pi} \cdot 0}{2 (\frac{1}{2})^{3/2}} e^{\frac{0^2}{- 4\frac{1}{2}}} \right) du_2 \dots du_d, \right. \\
& \qquad \qquad \qquad \dots, \\
& \qquad \qquad \qquad \left. \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty u_d\; e^{- \frac{u_2^2 + \cdots u_{d-1}^2}{2}} \left( \frac{1}{2}\sqrt{\frac{\pi}{1/2}} \right) du_2 \dots, du_d \right) \\
%
&  =  (2 \pi) ^{-d/2} \left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty e^{- \frac{u_2^2 + \cdots u_d^2}{2}} \left( 0 \right) du_2 \dots du_d, \right. \\
& \qquad \qquad \qquad \dots, \\
& \qquad \qquad \qquad \left. \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty u_d\; e^{- \frac{u_2^2 + \cdots u_{d-1}^2}{2}} \left( \sqrt{\frac{\pi}{2}} \right) du_2 \dots, du_d \right) \\
%
& = \cdots \\
& = (0, \dots, 0) \\
& = w_1
\end{align*}

\begin{align*}
p_{t+1}(u) & = \frac{p_t(u) e^{-\eta \langle u, \nabla f_t (w_t) \rangle}}{N_t} \\
& = \frac{p_{t-1}(u) e^{-\eta \langle u, \nabla f_{t-1} (w_{t-1}) \rangle} e^{-\eta \langle u, \nabla f_t (w_t) \rangle} }{N_{t-1} N_t} \\
& = p_1(u) \frac{e^{-\eta \langle u, \nabla f_{1} (w_{1}) \rangle} \cdots e^{-\eta \langle u, \nabla f_t (w_t) \rangle} }{N_{1} \cdots N_t} \\
& = p_1(u) \frac{e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle}}{N_{1} \cdots N_t} \\
& = (2 \pi) ^{-d/2} \frac{e^{- 1/2 \sum_{j = 1}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle}}{N_{1} \cdots N_t}
\end{align*}

\begin{align*}
\mu_{t + 1} & = \int_{\mathbb{R}^d} u \; (2 \pi) ^{-d/2} \frac{e^{- 1/2 \sum_{j = 1}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle}}{N_{1} \cdots N_t} du \\
\mu_{t + 1, k} & = \int_{\mathbb{R}^d} u_k \; (2 \pi) ^{-d/2} \frac{e^{- 1/2 \sum_{j = 1}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle}}{N_{1} \cdots N_t} du \\
& = \frac{(2 \pi) ^{-d/2}}{N_{1} \cdots N_t} \int_{\mathbb{R}^d} u_k \;  e^{- 1/2 \sum_{j = 1}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle} du \\
& = \frac{(2 \pi) ^{-d/2}}{N_{1} \cdots N_t} \int_{\mathbb{R}^{d-1}} \left( \int_{- \infty}^\infty u_k \;  e^{- 1/2 \sum_{j = 1}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \langle u, \nabla f_{i} (w_{i}) \rangle} du_1 \right) du_2 \dots du_d \\
& = \frac{(2 \pi) ^{-d/2}}{N_{1} \cdots N_t} \int_{\mathbb{R}^{d-1}} u_k \; e^{- 1/2 \sum_{j = 2}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \sum_{j = 2}^d u_j \cdot  \frac{\partial f_i}{\partial u_j}(w_{i})} \left( \int_{- \infty}^\infty e^{- 1/2 u_1^2} \; e^{-\eta \sum_{i = 1}^t u_1 \cdot  \frac{\partial f_i}{\partial u_1}(w_{i})} du_1 \right) du_2 \dots du_d \\
& = \frac{(2 \pi) ^{-d/2}}{N_{1} \cdots N_t} \int_{\mathbb{R}^{d-1}} u_k \; e^{- 1/2 \sum_{j = 2}^d u_j^2} \; e^{-\eta \sum_{i = 1}^t \sum_{j = 2}^d u_j \cdot  \frac{\partial f_i}{\partial u_j}(w_{i})} \left( \sqrt{2\pi} e^{\eta^2 (\sum_{i = 1}^t  \frac{\partial f_i}{\partial u_j}(w_{i} ))^2} \right) du_2 \dots du_d \\
& = \frac{(2 \pi) ^{-d/2}}{N_{1} \cdots N_t} \left( (\sqrt{2\pi})^{d-1} e^{\eta^2 \sum_{j = 1, j \neq k}^d (\sum_{i = 1}^t \frac{\partial f_i}{\partial u_j}(w_{i} ))^2} \right) \int_{-\infty}^\infty u_k \; e^{- 1/2 u_k^2} \; e^{-\eta \sum_{i = 1}^t u_k \cdot  \frac{\partial f_i}{\partial u_k}(w_{i})}  du_k \\
& = \frac{(2 \pi) ^{-1/2}}{N_{1} \cdots N_t} \left( e^{\eta^2 \sum_{j = 1, j \neq k}^d (\sum_{i = 1}^t \frac{\partial f_i}{\partial u_j}(w_{i} ))^2} \right) \left( \frac{\sqrt{\pi} (-\eta) \sum_{i = 1}^t \frac{\partial f_i}{\partial u_k}(w_{i})}{2 \frac{1}{2\sqrt{2}}} e^{\frac{\eta^2 (\sum_{i = 1}^t \frac{\partial f_i}{\partial u_k}(w_{i}))^2}{2}} \right) \\
& = \frac{1}{N_{1} \cdots N_t} \left( e^{\eta^2 \sum_{j = 1}^d (\sum_{i = 1}^t \frac{\partial f_i}{\partial u_j}(w_{i} ))^2} \right) \left( (-\eta) \sum_{i = 1}^t \frac{\partial f_i}{\partial u_k}(w_{i}) \right) \\
\end{align*}

\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}{\textbf{Strongly Convex Online To Batch Conversion}}

\begin{subquestion}{Consider loss functions of the form $f_t(u) = \frac{1}{2} (u - y_t)^2$ for $u, y_t \in \mathbb{R}$. Show that $f_t$ is strongly convex for degree $\alpha = 1$.}
\begin{solution}
For strongly convex function $f$ it holds:
\[
f(y) \geq f(x) +  \langle y - x , \nabla f(x) \rangle + \frac{\alpha}{2} || x - y ||^2
\]
Since, in our case $\alpha = 1$ and $f_t: \mathbb{R} \to \mathbb{R}$, we need to prove for any $u_2, u_1 \in \mathbb{R}$ the following:
\[
f_t(u_2) \geq f_t(u_1) +  (u_2 - u_1) \cdot f'_t(u_1) + \frac{1}{2} ( u_1 - u_2 )^2; \quad f'_t(u) = u - y_t
\]
So:
\begin{align*}
\frac{1}{2} (u_2 - y_t)^2 & \geq \frac{1}{2} (u_1 - y_t)^2 + (u_2 - u_1)(u_1 - y_t) + \frac{1}{2} ( u_1 - u_2 )^2 \\
(u_2 - y_t)^2 & \geq  (u_1 - y_t)^2 + 2(u_2 - u_1)(u_1 - y_t) +  ( u_1 - u_2 )^2 \\
u_2^2 - 2u_2 y_t + y_t^2 & \geq u_1^2 - 2 u_1 y_t + y_t^2 + 2 u_1 u_2 - 2u_2 y_t - 2 u_1^2 + 2 u_1 y_t + u_1^2 - 2u_1 u_2 + u_2^2 \\
0 & \geq 0
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Construct an estimator $\hat{w}_T(y_1, \dots, y_T)$ (by online to batch conversion) and show that its excess risk is at most
\[
\mathbb{E}_{y_1, \dots, y_T, y} \left[ \frac{1}{2} \left( \hat{w}_T(y_1, \dots, y_T) - y \right) ^2  - \frac{1}{2} (u^* - y)^2 \right] \leq \frac{1 + \ln T}{2T}
\]}
\end{subquestion}

\end{question}

\bibliographystyle{plain}
\bibliography{references}
\end{document}



















