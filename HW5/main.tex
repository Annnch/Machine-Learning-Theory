\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 5}
\author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
%\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{}

\begin{subquestion}{Consider a hypothesis class $\Hy = \cup_{n = 1}^{\infty} \Hy_n$, where for every $n \in \mathbb{N}$, $\Hy_n$ is finite. Find a weighting function $w: \Hy \to [0, 1]$ such that $\sum_{h \in \Hy} w(h) \leq 1$ and so that for all $h \in \Hy$, $w(h)$ is determined by $|\Hy_{n(h)}|$.}
\begin{solution}
Since we have a countably infinite union of finite sets, we know that the number of elements is countably infinite. Therefore, we can number them as:
\[
h_1, h_2, \dots
\]
If we pick weights as:
\[
w(h_i) = \left(\frac{1}{2^{|H_{n(h_i)}|}}\right)^i; \quad i = 1, 2, \dots
\]
the sum of the weights in the worst case would be, when $|H_{n(h_i)}| = 1$ ($\forall i$):
\[
\sum_{i = 1}^{\infty} w(h_i) = \sum_{i = 1}^{\infty} \left(\frac{1}{2}\right)^i = 1
\]
\end{solution}
\end{subquestion}

\begin{subquestion}{Define such a function $w$ when for all $n$ $\Hy_n$ is countable (possibly infinite).}
\begin{solution}
Countably infinite union of countable sets is again a countable set, so we can choose the same weighted function as before.
\end{solution}
\end{subquestion}

\end{question}


% 2nd question
\begin{question}{In this question we wish to show a No-Free-Lunch result for nonuniform learnability.}

\begin{subquestion}{Let $A$ be a nonuniform learner for class $\Hy$. For each $n \in \mathbb{N}$ define $\Hy_n^A = \{ h \in \Hy : m^{\text{NUL}} (0.1, 0.1, h) \leq n \}$. Prove that each such class $\Hy_n$ has a finite VC-dimension.}
\begin{solution}
	
Because $\Hy$ is nonuniform learnable, it is a union of agnostic PAC learnable hypothesis classes (Theorem 7.2). That is, $\Hy = \bigcup_{n \in \mathbb{N}} \Hy_n^A$, and $\Hy_n^A$ is agnostic PAC learnable for all $n \in \mathbb{N}$. Because each $\Hy_n^A$ is agnostic PAC learnable, it also has a finite VC-dimension, by the Fundamental Theorem of Statistical Learning (Theorem 6.7).

\end{solution}
\end{subquestion}

\begin{subquestion}{Prove that if class $\Hy$ in nonuniformly learnable then there are classes $\Hy_n$ so that $\Hy = \cup_{n \in \mathbb{N}} \Hy_n$ and, for every $n \in \mathbb{N}$, $\VC(\Hy_n)$ is finite.}
\begin{solution}
% https://www.cs.huji.ac.il/~shais/Lectures2014/lecture5.pdf
Let define:
\[
\Hy_n = \{ h \in \Hy : m^{\text{NUL}} (0.1, 0.1, h) \leq n \}
\]
It is obvious that $\Hy = \cup_{n \in \mathbb{N}} \Hy_n$. From previous point we also know that $\VC(\Hy_n)$ is finite.
\end{solution}
\end{subquestion}

\begin{subquestion}{Let $\Hy$ be a class that shatters some infinite set. Then for every sequence of classes ($\Hy_n : n \in \mathbb{N}$) such that $\Hy = \cup_{n \in \mathbb{N}} \Hy_n$, there exists some $n$ for which $\VC(\Hy_n) = \infty$.}

\begin{solution}
By assumption the hypothesis class $\Hy$ shatters some infinite set $K$. Let ($\Hy_n : n \in \mathbb{N}$) be a set of hypothesis classes, each having a finite VC-dimension. We define subsets $K_n \subseteq K$ such that, for all $n$, $|K_n| > \VC(H_n)$, and all subsets are nonoverlapping, so $K_n \cap K_m = \emptyset$.

Now, for each $K_n$ we pick a function $f_n : K_n \rightarrow \{0, 1\}$ so that no $h \in H_n$ agrees with $f_n$ on the domain $K_n$. Such a function exists for all $K_n$, because $H_n$ does not shatter $K_n$, since $|K_n| > \VC(H_n)$.

Next, we define the function $f: K \rightarrow \{0, 1\}$ by combining all $f_n$'s

$$
f(k)=
\begin{cases}
f_1(k) \;\; \text{if} \;\; k \in K_1    \\
\vdots \\
f_n(k) \;\; \text{if} \;\; k \in K_n
\end{cases}
$$

\noindent
where $k \in K$ are singletons in $K$.

We define $H_0 = \cup_{n \in \mathbb{N}} \Hy_n$. By the construction of all $f_n$'s it follows that $f$ is not contained in $H_0$. However, $f \in H$, because $H$ shatters $K$ by assumption. Put differently, $f \in (\Hy \setminus H_0)$.

This implies that $(\Hy \setminus H_0) \neq \emptyset$. And since we did not put any restrictions on $H_n$'s except $\VC(H_n) < \infty$, $(\Hy \setminus H_0)$ must contain at least one hypothesis class that has infinite VC-dimension, wich concludes the proof.
\end{solution}


\end{subquestion}

\begin{subquestion}{Construct a class $\Hy_1$ of functions from the unit interval $[0, 1]$ to $\{0, 1\}$ that is nonuniformly learnable but not PAC learnable.}
\begin{solution}
% http://www.cs.technion.ac.il/~itai/publications/Learning/nonuniform.pdf
Let denote:
\[
\Hy_n = \{ h_{a_1, \dots, a_n, b_1, \dots, b_n}; 0 \leq a_1 < b_1 \leq 1, \dots, 0 \leq a_n < b_n \leq 1 \}
\]
where:
\[
h_{a_1, \dots, a_n, b_1, \dots, b_n} (x) = 
\begin{cases}
1; & a_1 \leq x \leq b_1\ \text{or}\ \cdots\ \text{or}\ a_n \leq x \leq b_n \\
0; & \text{otherwise}
\end{cases}
\]
We claim that $\VC(\Hy_n) = 2n$. 

\begin{itemize}
	\item $\VC(\Hy_n) \geq 2n$: We would like to show that $\Hy_n$ shatters a set of $2n$ points. Assume that $0 \leq x_1 < \cdots < x_{2n} \leq 1$. Labeling for which we need the largest number of disjoint intervals is when $x$-es with even indexes are labeled 0 and others are labeled 1 (or vice versa). This is exactly $n$ intervals that we need, so $\Hy_n$ shatters $\{x_1, \dots, x_{2n}\}$.
	\item $\VC(\Hy_n) \leq 2n$: Now, we are going to take a set of $2n + 1$ elements; $0 \leq x_1 < \cdots < x_{2n + 1} \leq 1$. If we want that $\Hy_n$ shatters our set, there must exist a labeling function which can label the situation where $x$-es with odd indexes are labeled 1 and others are labeled 0. In order to do that we would need at least $n + 1$ intervals, which we do not have. So, $\Hy_n$ does not shatter a set of $2n + 1$ elements.
\end{itemize}

Denote $\Hy = \bigcup_{i = 1}^{\infty} \Hy_n$. From previous points we can conclude that $\Hy$ is nonuniformly learnable. It is not PAC learnable due to $\VC(\Hy) = \infty$.

\end{solution}
\end{subquestion}

\begin{subquestion}{Construct a class $\Hy_2$ of functions from the unit interval $[0, 1]$ to $\{0, 1\}$ that is not nonuniformly learnable.}
\begin{solution}
% http://www.cs.technion.ac.il/~itai/publications/Learning/nonuniform.pdf
Define $\Hy$ as a set of all intervals over $[0, 1]$. If is obvious that $\Hy$ shatters a set $S = \{\frac{1}{n};\ n \in \mathbb{N}\}$. We also know that $S$ is infinite. Therefore $\Hy$ is not nonuniformly learnable (from previous points).
\end{solution}
\end{subquestion}

\end{question}


% 3rd question
\begin{question}{Prove the Symmetrization Lemma (= double sample trick): \\
For any $\epsilon > 0$ such that $n\epsilon^2 \geq 2$, 
\[
\sup_{f \in \mathcal{F}} P_n \left[  (R(f) - R_n(f)) \geq \epsilon \right] \leq P_{2n} \left[ \sup_{f \in \mathcal{F}} (R'_n(f) - R_n(f)) \geq \frac{\epsilon}{2} \right]
\]
where $R$ is the risk, $R_n$ empirical risk for the sample $Z_1, \dots, Z_n$ and $R_n'$ empirical risk for ghost sample $Z_1', \dots, Z_n'$.}
\begin{solution}
This question was solved with the help of \cite{question3}.
% http://www.stat.cmu.edu/~larry/=sml/Concentration.pdf
Denote $f^*$ a function that maximize $(R(f) - R_n(f))$. First, we would like to prove: If $(R(f^*) - R_n(f^*) \geq \epsilon)$ and $(R(f^*) - R_n'(f^*) \leq \frac{\epsilon}{2})$ then $(R_n'(f^*) - R_n(f^*) \geq \frac{\epsilon}{2})$.
\begin{align*}
\epsilon & < R(f^*) - R_n(f^*) \\
& = R(f^*) - R_n'(f^*) + R_n'(f^*) - R_n(f^*) \\
& \leq R_n'(f^*) - R_n(f^*) + \frac{\epsilon}{2}
\end{align*}
So, $(R_n'(f^*) - R_n(f^*)) \geq \frac{\epsilon}{2}$. If we write that with indicators:
\[
I \left[ R(f^*) - R_n(f^*) > \epsilon, R(f^*) - R_n'(f^*) \leq \frac{\epsilon}{2} \right] \leq I \left[ R_n'(f^*) - R_n(f^*) \geq \frac{\epsilon}{2} \right]
\]
Now, we are going to compute expected value over $Z_1', \dots, Z_n'$:
\[
I \left[ R(f^*) - R_n(f^*) > \epsilon \right] P_n \left[ R(f^*) - R_n'(f^*) \leq \frac{\epsilon}{2} \right] \leq P_n \left[ R_n'(f^*) - R_n(f^*) \geq \frac{\epsilon}{2} \right]
\]
With Chebishyev inequality we get:
\[
P_n \left[ R(f^*) - R_n'(f^*) \leq \frac{\epsilon}{2} \right] \geq 1- \frac{4 Var(f^*)}{n\epsilon^2} \geq 1 - \frac{1}{n\epsilon^2} \geq \frac{1}{2}
\]
Therefore:
\[
I \left[ R(f^*) - R_n(f^*) \geq \epsilon \right]  \leq 2 P_n \left[ R_n'(f^*) - R_n(f^*) \geq \frac{\epsilon}{2} \right]
\]
Last, we again compute expectation but this time over $Z_1, \dots, Z_n$:
\[
P_n \left[ R(f^*) - R_n(f^*) \geq \epsilon \right]  \leq 2 P_{2n} \left[ R_n'(f^*) - R_n(f^*) \geq \frac{\epsilon}{2} \right]
\]
\end{solution}
\end{question}


\bibliographystyle{plain}
\bibliography{references}
\end{document}



















