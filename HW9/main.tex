\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 8}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{Let $\psi(\lambda) = \frac{\lambda^2}{2}$. The Legendre-Fenchel transform of $\psi$ is given by
\[
\psi^*(\epsilon) = \sup_{\lambda \in \mathbb{R}} \lambda \epsilon - \psi(\lambda).
\]}

\begin{subquestion}{$\psi^*(\epsilon) = \frac{\epsilon^2}{2}$.}
\begin{solution}
Let define $\psi_1(\lambda)$:
\[
\psi_1(\lambda) = \lambda \epsilon - \frac{\lambda^2}{2}
\]
Furthermore:
\[
\psi_1'(\lambda) = \epsilon - \lambda
\]
So, the maximum is reached at:
\[
\lambda = \epsilon \quad \Rightarrow \quad \psi_1(\epsilon) = \epsilon \cdot \epsilon - \frac{\epsilon^2}{2} = \frac{\epsilon^2}{2}
\]
We can conclude:
\[
\psi^*(\epsilon) = \psi_1(\epsilon) = \frac{\epsilon^2}{2}
\]
\end{solution}
\end{subquestion}

\begin{subquestion}{$(\psi^*)^{-1}(z) = \pm \sqrt{2z}$.}
\begin{solution}
From previous point we know:
\[
\psi^*(\epsilon) = \frac{\epsilon^2}{2}
\]
It follows:
\begin{align*}
z & = \frac{\epsilon^2}{2} \\
2 z & = \epsilon^2 \\
\epsilon & = \pm \sqrt{2z} 
\end{align*}
So:
\[
(\psi^*)^{-1}(z) = \pm \sqrt{2z}
\]
\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}{\textbf{The Blooper Reel}}

\begin{subquestion}{\textbf{Deterministic fails for Adversarial Bandits} Show that any deterministic algorithm (UCB included) has linear regret in the adversarial bandit setting. Hint: you can use the argument on the top of page 23.}
\end{subquestion}

\end{question}

% 3rd question
\begin{question}{We consider an adversarial bandit model with $K^2$ arms indexed by $i \in [K]$ and $j \in [K]$. For each arm $(i, j)$, the loss at time $t$ is $a_t^i + b_t^j$, where $a_t^i \in [0, 1]$ and $b_t^j \in [0, 1]$ are chosen by the adversary before the start of the interaction. Then each round the learner picks an arm $(I_t, J_t) \in [K]^2$ and observes $a_t^{I_t}$ and $b_t^{J_t}$ separately (and incurs their sum as the loss).}

\begin{subquestion}{Consider running a single instance of EXP3 on all $K^2$ arms (with
loss range $[0, 2]$). Show that the expected pseudo-regret compared to the best arm $(i^*, j^*)$  is bounded by
\[
\bar{R}_n \leq 2 \sqrt{2nK^2 \ln(K^2)}
\]}
\begin{solution}
\[
\bar{R}_n = \mathbb{E} \sum_{t=1}^T l_{t, I_t, J_t} - \min_{k_1, k_2} \sum_{t=1}^T l_{t, k_1, k_2}
\]
\[
\widetilde{l}_{t, i, j} = \frac{l_{t, i, j}}{p_{t, i, j}} \mathbbm{1}_{I_t = i, J_t = j} \quad \Rightarrow \quad \mathbb{E} \left[ \widetilde{l}_{t, i, j} \right] = l_{t, i, j}
\]
EXP3 algorithm:\\
learning rate $\eta$\\
set $p_{t, i, j} = \displaystyle \frac{exp(\eta \sum_{s = 1}^{t -1} \widetilde{l}_{t, i, j} )}{norm}$ \\
sample $I_t \sim p_t, J_t \sim p_t$ \\
set $\widetilde{l}_{t, i, j} = \frac{l_{t, i, j}}{p_{t, i, j}} \mathbbm{1}_{I_t = i, J_t = j} $\\
Theorem: $\bar{R}_T \leq \sqrt{2TK^2 \ln K^2}$ \\
Lemma: $\bar{R}_T \leq \frac{K^2T\eta}{2} + \frac{\log K^2}{\eta}$\\
Hadge analysis on $\widetilde{l}$
\end{solution}
\end{subquestion}

\begin{subquestion}{Now we will use the $a_t^i$ and $b_t^j$ observations separately. Consider
running two $K$-arm  instances of EXP3, one with $i \to a_t^i$ as the loss and one with $j \to b_t^j$ as the loss. Have the first algorithm control $I_t$ and the second $J_t$. Show that the overall expected pseudo-regret is bounded by
\[
\bar{R}_n \leq 2 \sqrt{2n K \ln K}.
\]}
\begin{solution}
From the lectures we know that the regret of one $K$-arm EKP3 algorithm is bounded with $\sqrt{2nK \ln K}$. So, in our case:
\[
\bar{R}_n = \bar{R}_n^1 + \bar{R}_n^2 \leq \sqrt{2nK \ln K} + \sqrt{2nK \ln K} = 2 \sqrt{2nK \ln K}
\]
\end{solution}
\end{subquestion}

\end{question}

\bibliographystyle{plain}
\bibliography{references}
\end{document}



















