\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MLT Homework 8}
% \author{Ana Borovac \\ Jonas Haslbeck \\ Bas Haver} % I'll be coming back :-)
\author{Ana Borovac  \\ Bas Haver}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}

\usepackage{url}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VC}{\text{VCdim}}

\newcounter{counterquestion}
\newenvironment{question}[1]
{
\stepcounter{counterquestion}
\section*{Question \thecounterquestion}
\emph{#1} 
} 
{
}

\newcounter{countersubquestion}[counterquestion]
\newenvironment{subquestion}[1]
{
\stepcounter{countersubquestion}
\subsection*{Subquestion \thecounterquestion .\thecountersubquestion}
\emph{#1} 
} 
{
}

\newenvironment{solution}
{
\subsubsection*{Solution}
} 
{
}


\begin{document}

\maketitle

% 1st question
\begin{question}{Let $\psi(\lambda) = \frac{\lambda^2}{2}$. The Legendre-Fenchel transform of $\psi$ is given by
\[
\psi^*(\epsilon) = \sup_{\lambda \in \mathbb{R}} \lambda \epsilon - \psi(\lambda).
\]}

\begin{subquestion}{$\psi^*(\epsilon) = \frac{\epsilon^2}{2}$.}
\begin{solution}
Let define $\psi_1(\lambda)$:
\[
\psi_1(\lambda) = \lambda \epsilon - \frac{\lambda^2}{2}
\]
Furthermore:
\[
\psi_1'(\lambda) = \epsilon - \lambda
\]
Since $\psi_1(\lambda)$ is a parabola it has only one extreme; particularly it has just a maximum (negative sign before $\lambda^2$). So, the maximum is reached at:
\[
\lambda = \epsilon \quad \Rightarrow \quad \psi_1(\epsilon) = \epsilon \cdot \epsilon - \frac{\epsilon^2}{2} = \frac{\epsilon^2}{2}
\]
We can conclude:
\[
\psi^*(\epsilon) = \psi_1(\epsilon) = \frac{\epsilon^2}{2}
\]
\end{solution}
\end{subquestion}

\begin{subquestion}{$(\psi^*)^{-1}(z) = \pm \sqrt{2z}$.}
\begin{solution}
From previous point we know:
\[
\psi^*(\epsilon) = \frac{\epsilon^2}{2}
\]
It follows:
\begin{align*}
z & = \frac{\epsilon^2}{2} \\
2 z & = \epsilon^2 \\
\epsilon & = \pm \sqrt{2z} 
\end{align*}
So:
\[
(\psi^*)^{-1}(z) = \pm \sqrt{2z}
\]
\end{solution}
\end{subquestion}

\end{question}

% 2nd question
\begin{question}{\textbf{The Blooper Reel}}

\begin{subquestion}{\textbf{Deterministic fails for Adversarial Bandits} Show that any deterministic algorithm (UCB included) has linear regret in the adversarial bandit setting. Hint: you can use the argument on the top of page 23.}
\end{subquestion}
\begin{solution}
We were a bit confused by this question, since it states that we need to find a linear regret, which we did not find. Instead we found that it can be bounded between two linear functions, but it does not necessarily is linear itself.\\
As a counter-example of the linearity we have that for $n=0$ no regret has been obtained. Therefore linearity only holds when $R_m+R_n=R_{n+m}$ for all $n,m$. But when we choose $n=4$, four arms and choose to play on arms 1,2,3 and 4 succesively, we find $R_1=1$, $R_3=3$, $R_4=3$, so linearity dos not hold. It does however hold that it remains between our bounds $\frac{n}{K}$ and $n$ for $K$ the amount of arms.\\
We use the hint on the top of page 23, which tells us that for a deterministic forecaster, we can use the following sequence of losses:
\begin{align*}
\text{if }I_t=1, \quad \text{then }l_{2,t}=0\quad \text{and}\quad l_{i,t}=1\quad \text{for all }i\neq 2;\\
\text{if }I_t\neq 1, \quad \text{then }l_{1,t}=0\quad \text{and}\quad l_{i,t}=1\quad \text{for all }i\neq 1
\end{align*}
This now implies the following for the regret:
\begin{align*}
R_n&=\sum _{t=1} ^n l_{t,I_t}-\min _k \sum _{t=1}^nl_n^k\\
&=n-\min _k \sum _{t=1}^nl_n^k
\end{align*}
But now for any choice of arm, with at least two arms, $\min _k \sum _{t=1}^nl_n^k$ is at most $\frac{n}{K}$. Therefore the regret is bounded from below by $\frac{n}{2}$ and since the loss function is nonnegative, the regret is also bounded from above by $n$. Thus it has a linear behaviour.
\end{solution}

\end{question}

% 3rd question
\begin{question}{We consider an adversarial bandit model with $K^2$ arms indexed by $i \in [K]$ and $j \in [K]$. For each arm $(i, j)$, the loss at time $t$ is $a_t^i + b_t^j$, where $a_t^i \in [0, 1]$ and $b_t^j \in [0, 1]$ are chosen by the adversary before the start of the interaction. Then each round the learner picks an arm $(I_t, J_t) \in [K]^2$ and observes $a_t^{I_t}$ and $b_t^{J_t}$ separately (and incurs their sum as the loss).}

\begin{subquestion}{Consider running a single instance of EXP3 on all $K^2$ arms (with
loss range $[0, 2]$). Show that the expected pseudo-regret compared to the best arm $(i^*, j^*)$  is bounded by
\[
\bar{R}_n \leq 2 \sqrt{2nK^2 \ln(K^2)}
\]}
\begin{solution}
Below we used the following facts:
\begin{itemize}
	\item $\min x + y = \min x + \min y;\ x, y \geq 0$
	\item Linearity of expected value.
	\item Theorem from the lectures: $\bar{R}_n \leq \sqrt{2n K \ln K}$, where $K$ is the number of arms; in our case we have $K^2$ arms.
\end{itemize}
\begin{align*}
\bar{R}_n & = \mathbb{E}_{I_1, \dots, I_n, J_1, \dots, J_n} \left\{ \sum_{t = 1}^n a_t^{I_t} + b_t^{J_t} \right\} - \min_k \sum_{t = 1}^n a_t^k + b_t^k \\
& = \left( \mathbb{E}_{I_1, \dots, I_n} \left\{ \sum_{t = 1}^n a_t^{I_t} \right\} - \min_k \sum_{t = 1}^n a_t^k \right) + \left( \mathbb{E}_{J_1, \dots, J_n} \left\{ \sum_{t = 1}^n b_t^{J_t} \right\} - \min_k \sum_{t = 1}^n b_t^k \right) \\
& \leq \sqrt{2n K^2 \ln K^2} + \sqrt{2n K^2 \ln K^2} \\
& = 2 \sqrt{2n K^2 \ln K^2} 
\end{align*}
\end{solution}
\end{subquestion}

\begin{subquestion}{Now we will use the $a_t^i$ and $b_t^j$ observations separately. Consider
running two $K$-arm  instances of EXP3, one with $i \to a_t^i$ as the loss and one with $j \to b_t^j$ as the loss. Have the first algorithm control $I_t$ and the second $J_t$. Show that the overall expected pseudo-regret is bounded by
\[
\bar{R}_n \leq 2 \sqrt{2n K \ln K}.
\]}
\begin{solution}
We do similar as before, just that now we choose arm in the set of $K$ arms and not $K^2$.
\begin{align*}
\bar{R}_n & = \mathbb{E}_{I_1, \dots, I_n, J_1, \dots, J_n} \left\{ \sum_{t = 1}^n a_t^{I_t} + b_t^{J_t} \right\} - \min_k \sum_{t = 1}^n a_t^k + b_t^k \\
& = \left( \mathbb{E}_{I_1, \dots, I_n} \left\{ \sum_{t = 1}^n a_t^{I_t} \right\} - \min_k \sum_{t = 1}^n a_t^k \right) + \left( \mathbb{E}_{J_1, \dots, J_n} \left\{ \sum_{t = 1}^n b_t^{J_t} \right\} - \min_k \sum_{t = 1}^n b_t^k \right) \\
& \leq \sqrt{2n K \ln K} + \sqrt{2n K \ln K} \\
& = 2 \sqrt{2n K \ln K} 
\end{align*}
\end{solution}
\end{subquestion}

\end{question}

\bibliographystyle{plain}
\bibliography{references}
\end{document}



















